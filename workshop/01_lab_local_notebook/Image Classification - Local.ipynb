{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Local Notebook\n",
    "\n",
    "***Supervised Learning with TensorFlow Keras: Image Classification on CIFAR-10 Dataset***\n",
    "\n",
    "This notebook works well with `Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)` or `Python 3 (Tensorflow 2.3 Python 3.7 GPU Optimized)` kernel on SageMaker Studio, or Jupyter notebook kernel with TensorFlow >=v2.3 notebook environment.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Contents:\n",
    "1. [Objective](objective)\n",
    "2. [Background](background)\n",
    "3. [Environment Setup](environment-setup)\n",
    "4. [Data Preparation](data-augmentation)\n",
    "5. [Testing Harness](testing-harness)\n",
    "6. [Model Training](model-training)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The purpose of the lab is to demonstrate how to train Image Classification model under local notebook environment, especially, when running the notebook in SageMaker Studio environment.\n",
    "\n",
    "Teaching in-depth deep learning approaches with TensorFlow Keras is outside this scope, and we hope that you may reuse the notebook to fast experiment your model backed by local compute resources for your future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "To do image classification to identify objects like airplane, automobile, bird, cat, deer, dog, frog, horse, ship and trunk. We shall use Deep Learning framework TensorFlow to train a CNN model backed by [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). \n",
    "\n",
    "CIFAR is an acronym that stands for the [Canadian Institute For Advanced Research](https://cifar.ca/) and the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) was developed along with the CIFAR-100 dataset by researhers at the CIFAR instite.\n",
    "\n",
    "For [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), it consists of 60,000 32x32 pixel color pictures of objects from 10 classes, such as bird, cat and deer, etc. The class labels and their standard associated integer values are listed below:\n",
    "\n",
    "* 0: airplane\n",
    "* 1: automobile\n",
    "* 2: bird\n",
    "* 3: cat\n",
    "* 4: deer\n",
    "* 5: dog\n",
    "* 6: frog\n",
    "* 7: horse\n",
    "* 8: ship\n",
    "* 9: truck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We shall use CIFAR10 dataset from AWS Open Data - [Image classification - fast.ai datasets](https://registry.opendata.aws/fast-ai-imageclas/). Below are the step to setup the notebook running environment.\n",
    "\n",
    "***Please wait until the below cell execution done and kernel restarted***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip --user\n",
    "!{sys.executable} -m pip install matplotlib ipywidgets opencv-python seaborn sklearn --user\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We shall download CIFAR10 dataset to local `/tmp/data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/tmp/data'\n",
    "\n",
    "train_data_dir = f'{data_folder}/cifar10/train'\n",
    "test_data_dir = f'{data_folder}/cifar10/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "\n",
    "mkdir -p $data_folder\n",
    "\n",
    "aws s3 cp --no-sign-request s3://fast-ai-imageclas/cifar10.tgz $data_folder\n",
    "    \n",
    "ls -l $data_folder/cifar10.tgz\n",
    "\n",
    "tar -zxvf $data_folder/cifar10.tgz -C $data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile', \n",
    "    'bird',\n",
    "    'cat', \n",
    "    'deer', \n",
    "    'dog', \n",
    "    'frog', \n",
    "    'horse', \n",
    "    'ship', \n",
    "    'truck'\n",
    "]\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "nb_classes = len(class_names)\n",
    "\n",
    "IMAGE_SIZE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_image_array(image_path):\n",
    "    img = Image.open(image_path, 'r')\n",
    "    return np.array(img)\n",
    "\n",
    "def display_examples(class_names, dataset_folder):\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    fig.suptitle('Some examples of images of the dataset', fontsize = 16)\n",
    "    \n",
    "    for i in range(20):\n",
    "        image_class = class_names[random.randint(0, len(class_names) - 1)]\n",
    "        image_folder = f'{dataset_folder}/{image_class}'\n",
    "        image_files = os.listdir(image_folder)\n",
    "        image_file = image_files[random.randint(0, len(image_files) - 1)]\n",
    "        image_file_path = f'{image_folder}/{image_file}'\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(get_image_array(image_file_path))\n",
    "        plt.xlabel(image_class)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_examples(class_names, train_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Image augmentation applies transforms to an image and results in additional images that the network can train on. Image data generator has many options and also allows custom preprocessing functions through the parameter of the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(99)\n",
    "\n",
    "def blur_preprocessing(img):\n",
    "    return cv2.blur(img, (5, 5))\n",
    "\n",
    "# training data generator; commented parameters are not needed after evaluated the validation dataset.\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0, \n",
    "    validation_split = 0.0, # it's for training dataset only\n",
    "#     rotation_range = 30,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range =0.2,\n",
    "#     width_shift_range = 0.2,\n",
    "#     height_shift_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "#     vertical_flip = False,\n",
    "#     brightness_range = [0.5, 1.5],\n",
    "#     fill_mode = 'nearest',\n",
    "#     preprocessing_function = blur_preprocessing\n",
    ")\n",
    "\n",
    "# training dataset\n",
    "train_data_multi = train_data_generator.flow_from_directory(\n",
    "    directory = train_data_dir,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# testing data generator\n",
    "validation_data_generator = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0\n",
    ")\n",
    "\n",
    "# testing dataset\n",
    "validation_data_multi = validation_data_generator.flow_from_directory(\n",
    "    directory = test_data_dir,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_multi.class_indices, validation_data_multi.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience = 2, mode = 'min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "history = model.fit(\n",
    "    train_data_multi, \n",
    "    batch_size = 32,\n",
    "    epochs = 100,\n",
    "    validation_data = validation_data_multi,\n",
    "    verbose = 2,\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Evaluate the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "    axs[0].plot(history.history['accuracy'])\n",
    "    axs[0].plot(history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'validate'], loc = 'upper left')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc = 'upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = list(history.history)\n",
    "val_metric_names = [name for name in metric_names if name.startswith('val_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"val_abc\".start_with('val_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_loss'][-1], history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "        \"Validation results: \"\n",
    "        + \"; \".join(map(\n",
    "            lambda i: f\"{metric_names[i]}={history.history[metric_names[i]][-1]:.5f}\", range(len(metric_names))\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score trained model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score trained model\n",
    "scores = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./cat.png\", 1)\n",
    "\n",
    "# resize, as our model is expecting images in 32x32.\n",
    "image = cv2.resize(image, (32, 32))\n",
    "image = image / 255.0\n",
    "\n",
    "image = np.expand_dims(image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
